# Probability Distributions, Bayesian Probability, and Gradient Descent Implementation
This repo contains assignment of Probability Distributions, Bayesian Probability, and Gradient Descent Implementation
## Contributors
### David Niyonshuti: Worked on Part 1: Probability distribution
 * As I was working on my part of understanding normal probability distribution, I learned that when you have a continoues numerical data with missing values, you can use mean imputation to fill in the missing values, this helps in data preprocessing before you model your data. In addition, understanding normal distribution helps me to differ it with other probability distribution to know their use cases in their respective scenario depending on what decision you want to make depending on dataset you have. 
### Chol Danieal Deng Dau:
* As I was working on my part of understanding Bayesian probability through the example of planning a family picnic, I learned that Bayesian inference provides a structured way to update beliefs based on new evidence. This is particularly useful in scenarios where uncertainty exists, such as predicting weather conditions for outdoor activities. By incorporating prior knowledge (historical rain probabilities) and likelihood (accuracy of weather forecasts), we can compute posterior probabilities that help make informed decisions.

* In addition, understanding Bayesian probability helps me differentiate it from other probabilistic approaches, such as frequentist methods. While frequentist methods rely solely on observed data, Bayesian methods allow us to integrate prior information, making them more suitable for real-world problems where data might be limited or uncertain. This approach is especially valuable in decision-making processes like medical diagnosis, risk assessment, and even everyday situations like planning events based on weather forecasts. Depending on the dataset and the context, Bayesian analysis enables us to refine our predictions iteratively as new evidence becomes available, leading to more accurate and reliable conclusions.






  
### Abiodun Israel Kumuyi:
  
### Jean Pierre Niyongabo:
* Regarding the part I was working on, applying Bayesian probability to a real-world problem, I understood the concept very well. I compared it to the Monty Hall problem you showed us a few days ago, and I found that the Monty Hall problem is conditional probability. This type of probability is all about finding the chance of something happening because something else has already happened, while Bayesian probability is a way to update your beliefs about something when you get new information.
### Reponse Ashimwe Sibomana:
  
* In Part 3 of the task, we first implemented the manual calculation for parameter estimation, but even after multiple iterations, the MSE remained relatively high. Later, we used Scipyâ€™s built-in optimization, and I was impressed by how quickly and accurately it converged to a much lower MSE. Seeing the difference in efficiency between the two approaches made me understand the power of numerical libraries in handling complex computations. This experience reinforced the importance of leveraging optimized tools for accuracy and efficiency in machine learning tasks.
